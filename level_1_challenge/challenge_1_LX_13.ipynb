{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5652ce4c-c75d-4484-a43a-31ffcf7a2b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supress Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Geospatial\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import zarr # Not referenced, but required for xarray\n",
    "\n",
    "# Import Planetary Computer tools\n",
    "import fsspec\n",
    "import pystac\n",
    "\n",
    "# Other\n",
    "import os\n",
    "import zipfile\n",
    "from itertools import cycle\n",
    "\n",
    "# Path to data folder with provided material\n",
    "data_path = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ef421-3dea-4894-a605-d1695657335b",
   "metadata": {},
   "source": [
    "## Response Variable\n",
    "\n",
    "Before we can build our model, we need to load in the frog occurrences data and generate our response variable. To do this, we first need to unzip the training data and store it on our machine. Then we can write a function that abstracts the loading process, with the option of providing a bounding box to only take those occurrences within a region of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf305264-0afd-40c3-a9b1-3b782593ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_path+'training_data/'):\n",
    "    os.mkdir(data_path+'training_data/')\n",
    "    with zipfile.ZipFile(data_path+'GBIF_training_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path+'training_data/')\n",
    "        \n",
    "def filter_bbox(frogs, bbox):\n",
    "    frogs = frogs[lambda x: \n",
    "        (x.decimalLongitude >= bbox[0]) &\n",
    "        (x.decimalLatitude >= bbox[1]) &\n",
    "        (x.decimalLongitude <= bbox[2]) &\n",
    "        (x.decimalLatitude <= bbox[3])\n",
    "    ]\n",
    "    return frogs\n",
    "\n",
    "def get_frogs(file, year_range=None, bbox=None):\n",
    "    \"\"\"Returns the dataframe of all frog occurrences for the bounding box specified.\"\"\"\n",
    "    columns = [\n",
    "        'gbifID','eventDate','country','continent','stateProvince',\n",
    "        'decimalLatitude','decimalLongitude','species'\n",
    "    ]\n",
    "    country_names = {\n",
    "        'AU':'Australia', 'CR':'Costa Rica', 'ZA':'South Africa','MX':'Mexico','HN':'Honduras',\n",
    "        'MZ':'Mozambique','BW':'Botswana','MW':'Malawi','CO':'Colombia','PA':'Panama','NI':'Nicaragua',\n",
    "        'BZ':'Belize','ZW':'Zimbabwe','SZ':'Eswatini','ZM':'Zambia','GT':'Guatemala','LS':'Lesotho',\n",
    "        'SV':'El Salvador', 'AO':'Angola', np.nan:'unknown or invalid'\n",
    "    }\n",
    "    continent_names = {\n",
    "        'AU':'Australia', 'CR':'Central America', 'ZA':'Africa','MX':'Central America','HN':'Central America',\n",
    "        'MZ':'Africa','BW':'Africa','MW':'Africa','CO':'Central America','PA':'Central America',\n",
    "        'NI':'Central America','BZ':'Central America','ZW':'Africa','SZ':'Africa','ZM':'Africa',\n",
    "        'GT':'Central America','LS':'Africa','SV':'Central America','AO':'Africa', np.nan:'unknown or invalid' \n",
    "    }\n",
    "    frogs = (\n",
    "        pd.read_csv(data_path+'training_data/occurrence.txt', sep='\\t', parse_dates=['eventDate'])\n",
    "        .assign(\n",
    "            country =  lambda x: x.countryCode.map(country_names),\n",
    "            continent =  lambda x: x.countryCode.map(continent_names),\n",
    "            species = lambda x: x.species.str.title()\n",
    "        )\n",
    "        [columns]\n",
    "    )\n",
    "    if year_range is not None:\n",
    "        frogs = frogs[lambda x: \n",
    "            (x.eventDate.dt.year >= year_range[0]) & \n",
    "            (x.eventDate.dt.year <= year_range[1])\n",
    "        ]\n",
    "    if bbox is not None:\n",
    "        frogs = filter_bbox(frogs, bbox)\n",
    "    return frogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47010498-24fb-42b1-8ccc-3f7f89235c59",
   "metadata": {},
   "source": [
    "\n",
    "### Sub-sampling\n",
    "\n",
    "For this demonstration, we search for frogs in the Greater Sydney area and the 5 testing regions found between the start of 2015 to the end of 2019. This is done by providing `year_range` and `bbox` parameters to the get_frogs function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d75a397-ae04-48ae-89bd-406a209be3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxs=[\n",
    "    # Evaluation ones\n",
    "    (144.8,-38.5,145.8,-37.5), \n",
    "    (150.7,-33.5,151.7,-32.5), \n",
    "    (152.6,-29.0,153.6,-28.0),\n",
    "    (145.0,-17.7,146.0,-16.7),\n",
    "    (115.7,-32.5,116.7,-31.5),\n",
    "    # New ones here\n",
    "    (148.8, -35.7, 149.8, -34.7),\n",
    "    (152.3, -30.7, 153.3, -29.7),\n",
    "    (152.1, -27.3 , 153.1, -26.3),\n",
    "    (130.8, -12.8, 131.8, -11.8),\n",
    "    \n",
    "]\n",
    "    \n",
    "# Load in data\n",
    "\n",
    "frog_datas = []\n",
    "for bbox in bboxs:\n",
    "    frog_datas.append(get_frogs(data_path+'/training_data/occurrence.txt', year_range=(2015, 2022), bbox=bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f88bd1-b209-4c07-9e20-5a8dac90aec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gbifID</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>stateProvince</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>species</th>\n",
       "      <th>occurrenceStatus</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1633803922</td>\n",
       "      <td>2017-08-05 20:51:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.045122</td>\n",
       "      <td>145.326047</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3384896595</td>\n",
       "      <td>2021-10-07 07:58:39</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.030947</td>\n",
       "      <td>145.432053</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3463456338</td>\n",
       "      <td>2018-11-09 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.088716</td>\n",
       "      <td>145.722909</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3463456499</td>\n",
       "      <td>2019-07-30 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-37.807553</td>\n",
       "      <td>145.263607</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3463456516</td>\n",
       "      <td>2020-08-23 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-37.991842</td>\n",
       "      <td>145.118825</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5289</th>\n",
       "      <td>3130807827</td>\n",
       "      <td>2015-06-02 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-37.566500</td>\n",
       "      <td>145.088590</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>5289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>3130905773</td>\n",
       "      <td>2016-11-21 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.008890</td>\n",
       "      <td>145.149170</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>5290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>3129207916</td>\n",
       "      <td>2017-12-20 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-37.747250</td>\n",
       "      <td>145.085730</td>\n",
       "      <td>Litoria Fallax</td>\n",
       "      <td>1</td>\n",
       "      <td>5291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5292</th>\n",
       "      <td>3130850457</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-37.924810</td>\n",
       "      <td>145.556440</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>5292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>3130173222</td>\n",
       "      <td>2016-03-03 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.205600</td>\n",
       "      <td>145.473220</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>5293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5294 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gbifID           eventDate    country  continent stateProvince  \\\n",
       "0     1633803922 2017-08-05 20:51:00  Australia  Australia      Victoria   \n",
       "1     3384896595 2021-10-07 07:58:39  Australia  Australia      Victoria   \n",
       "2     3463456338 2018-11-09 00:00:00  Australia  Australia      Victoria   \n",
       "3     3463456499 2019-07-30 00:00:00  Australia  Australia      Victoria   \n",
       "4     3463456516 2020-08-23 00:00:00  Australia  Australia      Victoria   \n",
       "...          ...                 ...        ...        ...           ...   \n",
       "5289  3130807827 2015-06-02 00:00:00  Australia  Australia      Victoria   \n",
       "5290  3130905773 2016-11-21 00:00:00  Australia  Australia      Victoria   \n",
       "5291  3129207916 2017-12-20 00:00:00  Australia  Australia      Victoria   \n",
       "5292  3130850457 2019-01-01 00:00:00  Australia  Australia      Victoria   \n",
       "5293  3130173222 2016-03-03 00:00:00  Australia  Australia      Victoria   \n",
       "\n",
       "      decimalLatitude  decimalLongitude           species  occurrenceStatus  \\\n",
       "0          -38.045122        145.326047  Crinia Signifera                 0   \n",
       "1          -38.030947        145.432053  Crinia Signifera                 0   \n",
       "2          -38.088716        145.722909  Crinia Signifera                 0   \n",
       "3          -37.807553        145.263607  Crinia Signifera                 0   \n",
       "4          -37.991842        145.118825  Crinia Signifera                 0   \n",
       "...               ...               ...               ...               ...   \n",
       "5289       -37.566500        145.088590  Crinia Signifera                 0   \n",
       "5290       -38.008890        145.149170  Crinia Signifera                 0   \n",
       "5291       -37.747250        145.085730    Litoria Fallax                 1   \n",
       "5292       -37.924810        145.556440  Crinia Signifera                 0   \n",
       "5293       -38.205600        145.473220  Crinia Signifera                 0   \n",
       "\n",
       "       key  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  \n",
       "...    ...  \n",
       "5289  5289  \n",
       "5290  5290  \n",
       "5291  5291  \n",
       "5292  5292  \n",
       "5293  5293  \n",
       "\n",
       "[5294 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_species = 'Litoria Fallax'\n",
    "\n",
    "for i, frog_data in enumerate(frog_datas):\n",
    "    frog_datas[i] = (\n",
    "        frog_data\n",
    "        # Assign the occurrenceStatus to 1 for the target species and 0 for all other species.\n",
    "        # as well as a key for joining (later)\n",
    "        .assign(\n",
    "            occurrenceStatus = lambda x: np.where(x.species == target_species, 1, 0)\n",
    "        ).reset_index(drop=True)\n",
    "        .assign(key=lambda x: x.index)\n",
    "    )\n",
    "frog_datas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c6b93-39bd-4032-b601-56bb8b7cbf92",
   "metadata": {},
   "source": [
    "#### Spatial sampling\n",
    "\n",
    "While we have restricted our analysis to Greater Sydney for this demonstration,  you are encouraged to explore different areas to assist in creating a SDM that is representative of the habitat of litoria fallax. You could even use the entire training set, but keep in mind that loading in large areas may be quite computationally expensive. We recommend loading data in chunks that are a similar size to the bounding box we defined above.\n",
    "\n",
    "#### Temporal sampling\n",
    "\n",
    "Another area that may assist you in developing your SDM is the time dimension of the data. Each occurrence has an \"eventDate\" attribute, and the terraclimate data is taken monthly. You may want to consider whether more closely matching occurrences to timely data will improve your model. Another idea is to pool the data into larger time chunks like we have done in this notebook. This could involve extending our approach of binning the data in 5 year intervals to include occurrence data for 2010-2015, 2005-2009, etc. Either approach would allow you to utilise more of the training data which could greatly assist your SDM training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41413967-761e-46ab-9b2c-310532e8537a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Addressing bias\n",
    "\n",
    "Below we define some functions to assist in plotting the frog data. This will assist us in identifying two main areas of bias. We then use these functions to plot the frog species distributions of each country. A more detailed exploration of the training dataset for this challenge can be found in the [dataset summary notebook](supplementary_notebooks/dataset_summary.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6de119-ec0c-4428-abf2-05bd8d9948ed",
   "metadata": {},
   "source": [
    "#### Sampling bias\n",
    "\n",
    "The plot above shows how frog occurrences are heavily biased around urban areas, where people are more likely to come across them. They also cluster tightly around towns, parks, bush trails etc. This is one issue that would be worth addressing to maximise success in this challenge.\n",
    "\n",
    "One method of addressing the sampling bias inherent in the database is to use the occurrence points of other species as absence points for the target species. This is called pseudo-absence and is a common technique in species distribution modelling. This way, if a different species of frog has been sighted in a specific location, we can be more certain that the species we are trying to predict is not at that same location. Alternatively, if we just picked a random point where there are no frog occurrences, we cannot be certain that frogs are not in that location. It might just be that there are no walking tracks near that location, and therefore the frogs would not show up in our database.\n",
    "\n",
    "For this notebook, we will use the other species, crinia signifera - the common eastern froglet, as examples of litoria fallax's absence. We will alter our response variable to be `occurenceStatus` which will take the value of 1 if the occurrence species is litoria fallax, and 0 if the species is not litoria fallax (i.e. is crinia signifera)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd4236-b9a9-4622-9a59-2a3365ae9a01",
   "metadata": {},
   "source": [
    "## Predictor Variables\n",
    "\n",
    "Now that we have our response variable, it is time to gather the predictor variables from the TerraClimate dataset. For a more in-depth look at the TerraClimate dataset and how to query it, see the [TerraClimate supplementary notebook](./supplementary_notebooks/TerraClimate.ipynb)\n",
    "\n",
    "### Accessing the TerraClimate Data\n",
    "\n",
    "To get the TerraClimate data, we write a function called `get_terraclimate`. This function will fetch all data intersecting with the bounding box and will calculate various metrics over the time dimension for each coordinate. In this example, we will take four metrics from four assets, namely the mean maximum monthly air temp (`tmax_mean`), mean minimum monthly air temp (`tmin_mean`), mean accumulated precipitation (`ppt_mean`) and mean soil moisture (`soil_mean`), all calculated over a five year timeframe from the start of 2015 to the end of 2019.\n",
    "\n",
    "To assist in visualisations, this function has an interpolation functionality which will allow the comparitively coarse temporal resolution of the terraclimate data to be mapped to a larger set of coordinates, creating an ($n$ x $m$) image. We will choose (512 x 512).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895f4351-847a-4c66-a764-f9a0888b9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terraclimate_all(bbox, time_slice=None, assets=None, interp_dims=None, verbose=True):\n",
    "    \"\"\"Returns terraclimate data.\n",
    "    \n",
    "    Attributes:\n",
    "    bbox -- Tuple of (min_lon, min_lat, max_lon, max_lat) to define area\n",
    "    time_slice -- Tuple of datetime strings to select data between, e.g. ('2015-01-01','2019-12-31')\n",
    "    assets -- list of terraclimate assets to take\n",
    "    \"\"\"\n",
    "    min_lon, min_lat, max_lon, max_lat = bbox\n",
    "    \n",
    "    collection = pystac.read_file(\"https://planetarycomputer.microsoft.com/api/stac/v1/collections/terraclimate\")\n",
    "    asset = collection.assets[\"zarr-https\"]\n",
    "    store = fsspec.get_mapper(asset.href)\n",
    "    data = xr.open_zarr(store, **asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "    \n",
    "    # Select datapoints that overlap region\n",
    "    if time_slice is not None:\n",
    "        data = data.sel(lon=slice(min_lon,max_lon),lat=slice(max_lat,min_lat),time=slice(time_slice[0],time_slice[1]))\n",
    "    else:\n",
    "        data = data.sel(lon=slice(min_lon,max_lon),lat=slice(max_lat,min_lat))\n",
    "    if assets is not None:\n",
    "        data = data[assets]\n",
    "    print('Loading data') if verbose else None\n",
    "    data = data.rename(lat='y', lon='x').compute()\n",
    "    \n",
    "    if interp_dims is not None:\n",
    "        print(f'Interpolating image') if verbose else None\n",
    "        interp_coords = (np.linspace(bbox[0], bbox[2], interp_dims[0]), np.linspace(bbox[1], bbox[3], interp_dims[1]))\n",
    "        data = data.interp(x=interp_coords[0], y=interp_coords[1], method='nearest', kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_terraclimate_months(bbox, time_slice=None, assets=None, interp_dims=None, verbose=True):\n",
    "    \"\"\"Get weather data and take mean by month.\"\"\"\n",
    "    ds = get_terraclimate_all(bbox, time_slice=time_slice, assets=assets, interp_dims=interp_dims, verbose=verbose)\n",
    "    ds_month = ds.groupby('time.month').mean(dim='time')\n",
    "    \n",
    "    return ds_month\n",
    "\n",
    "def get_terraclimate(bbox, time_slice=None, assets=None, interp_dims=None, verbose=True):\n",
    "    \"\"\"Get weather data and take mean by month and make each month/asset a new variable.\"\"\"\n",
    "    ds_month = get_terraclimate_months(bbox, time_slice=time_slice, assets=assets, interp_dims=interp_dims, verbose=verbose)\n",
    "    sets = [\n",
    "        ds_month.sel(month=month\n",
    "                    ).rename(\n",
    "            {var: ' '.join((str(month), var)) for var in list(ds_month.keys())}\n",
    "        ).drop_vars('month')\n",
    "        for month in ds_month.month.values\n",
    "    ]\n",
    "    ds = xr.merge(sets).to_array().rename(variable='band')\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de76f8cc-2155-4f51-8ae9-ec79f87b764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n",
      "Loading data\n",
      "Interpolating image\n"
     ]
    }
   ],
   "source": [
    "# Date range to take\n",
    "time_slice = ('2015-01-01','2022-12-31')\n",
    "\n",
    "# Measurements to take\n",
    "assets=['tmax', 'tmin', 'ppt', 'soil']\n",
    "\n",
    "interp_dims = (512, 512)\n",
    "\n",
    "weather_datas = []\n",
    "for bbox in bboxs:\n",
    "    weather_datas.append(get_terraclimate(bbox, time_slice=time_slice, assets=assets, interp_dims=interp_dims, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e65acf-b700-43a3-9c39-d0f40a87e616",
   "metadata": {},
   "source": [
    "Below, we define the products to take from TerraClimate in `assets` and the metrics to calculate from them in `tc_metrics`. Each metric is applied to each asset, so to pick the desired asset/metric pairs we define a list of strings in the form '\\<asset\\>_\\<metric\\>' in `features`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a3347-a22d-4369-ae28-131326f72bc5",
   "metadata": {},
   "source": [
    "The spatial distribution of the four variables are displayed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd00439-c77f-4aaf-9cf7-274457de8b2d",
   "metadata": {},
   "source": [
    "The frequency distribution of each variable is displayed below. There is some skewness present in a few variables, so you might want to address this when training your own model. Depending on the type of model you decide to train, some of the variables might require normalisation, standardisation, or transformation. For now, we will proceed with the variables as they come."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0774bc5-4dff-4812-a1c6-70768abca0c9",
   "metadata": {},
   "source": [
    "### Joining Pretictors to the Response Variable\n",
    "\n",
    "Now that we have read in our predictor variables, we need to join them onto the response variable of frogs. To do this, we loop through the frog occurrence data and assign each frog occurrence the closest predictor pixel value from each of the predictor variables based on the geo-coordinates. The `sel` method of the xarray dataarray comes in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b397daf8-cb09-4a57-8317-e3a7d13a2255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gbifID</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>stateProvince</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>species</th>\n",
       "      <th>occurrenceStatus</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>7 tmax</th>\n",
       "      <th>7 tmin</th>\n",
       "      <th>8 ppt</th>\n",
       "      <th>8 soil</th>\n",
       "      <th>8 tmax</th>\n",
       "      <th>8 tmin</th>\n",
       "      <th>9 ppt</th>\n",
       "      <th>9 soil</th>\n",
       "      <th>9 tmax</th>\n",
       "      <th>9 tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1633803922</td>\n",
       "      <td>2017-08-05 20:51:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.045122</td>\n",
       "      <td>145.326047</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.700002</td>\n",
       "      <td>5.940001</td>\n",
       "      <td>81.199997</td>\n",
       "      <td>114.199997</td>\n",
       "      <td>14.120001</td>\n",
       "      <td>6.080003</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>16.540001</td>\n",
       "      <td>7.180002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3384896595</td>\n",
       "      <td>2021-10-07 07:58:39</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.030947</td>\n",
       "      <td>145.432053</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13.540001</td>\n",
       "      <td>5.660004</td>\n",
       "      <td>99.199997</td>\n",
       "      <td>183.600006</td>\n",
       "      <td>13.820002</td>\n",
       "      <td>5.560004</td>\n",
       "      <td>87.599998</td>\n",
       "      <td>183.600006</td>\n",
       "      <td>16.080000</td>\n",
       "      <td>6.600001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3463456338</td>\n",
       "      <td>2018-11-09 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-38.088716</td>\n",
       "      <td>145.722909</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>13.760004</td>\n",
       "      <td>5.140001</td>\n",
       "      <td>87.400002</td>\n",
       "      <td>149.600006</td>\n",
       "      <td>14.260000</td>\n",
       "      <td>5.379999</td>\n",
       "      <td>82.599998</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>6.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3463456499</td>\n",
       "      <td>2019-07-30 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-37.807553</td>\n",
       "      <td>145.263607</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>13.480001</td>\n",
       "      <td>5.480000</td>\n",
       "      <td>90.800003</td>\n",
       "      <td>124.400002</td>\n",
       "      <td>14.000002</td>\n",
       "      <td>5.480002</td>\n",
       "      <td>82.599998</td>\n",
       "      <td>119.199997</td>\n",
       "      <td>16.480000</td>\n",
       "      <td>6.560002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3463456516</td>\n",
       "      <td>2020-08-23 00:00:00</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>-37.991842</td>\n",
       "      <td>145.118825</td>\n",
       "      <td>Crinia Signifera</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>14.040001</td>\n",
       "      <td>6.360001</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>14.720001</td>\n",
       "      <td>6.640002</td>\n",
       "      <td>57.799999</td>\n",
       "      <td>49.799999</td>\n",
       "      <td>17.160002</td>\n",
       "      <td>7.740001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gbifID           eventDate    country  continent stateProvince  \\\n",
       "0  1633803922 2017-08-05 20:51:00  Australia  Australia      Victoria   \n",
       "1  3384896595 2021-10-07 07:58:39  Australia  Australia      Victoria   \n",
       "2  3463456338 2018-11-09 00:00:00  Australia  Australia      Victoria   \n",
       "3  3463456499 2019-07-30 00:00:00  Australia  Australia      Victoria   \n",
       "4  3463456516 2020-08-23 00:00:00  Australia  Australia      Victoria   \n",
       "\n",
       "   decimalLatitude  decimalLongitude           species  occurrenceStatus  key  \\\n",
       "0       -38.045122        145.326047  Crinia Signifera                 0    0   \n",
       "1       -38.030947        145.432053  Crinia Signifera                 0    1   \n",
       "2       -38.088716        145.722909  Crinia Signifera                 0    2   \n",
       "3       -37.807553        145.263607  Crinia Signifera                 0    3   \n",
       "4       -37.991842        145.118825  Crinia Signifera                 0    4   \n",
       "\n",
       "   ...     7 tmax    7 tmin      8 ppt      8 soil     8 tmax    8 tmin  \\\n",
       "0  ...  13.700002  5.940001  81.199997  114.199997  14.120001  6.080003   \n",
       "1  ...  13.540001  5.660004  99.199997  183.600006  13.820002  5.560004   \n",
       "2  ...  13.760004  5.140001  87.400002  149.600006  14.260000  5.379999   \n",
       "3  ...  13.480001  5.480000  90.800003  124.400002  14.000002  5.480002   \n",
       "4  ...  14.040001  6.360001  69.000000   52.799999  14.720001  6.640002   \n",
       "\n",
       "       9 ppt      9 soil     9 tmax    9 tmin  \n",
       "0  72.800003  116.000000  16.540001  7.180002  \n",
       "1  87.599998  183.600006  16.080000  6.600001  \n",
       "2  82.599998  159.000000  16.620001  6.720000  \n",
       "3  82.599998  119.199997  16.480000  6.560002  \n",
       "4  57.799999   49.799999  17.160002  7.740001  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_frogs(frogs, data):\n",
    "    \"\"\"Collects the data for each frog location and joins it onto the frog data \n",
    "\n",
    "    Arguments:\n",
    "    frogs -- dataframe containing the response variable along with [\"decimalLongitude\", \"decimalLatitude\", \"key\"]\n",
    "    data -- xarray dataarray of features, indexed with geocoordinates\n",
    "    \"\"\"\n",
    "    return frogs.merge(\n",
    "        (\n",
    "            data\n",
    "            .rename('data')\n",
    "            .sel(\n",
    "                x=xr.DataArray(frog_data.decimalLongitude, dims=\"key\", coords={\"key\": frog_data.key}), \n",
    "                y=xr.DataArray(frog_data.decimalLatitude, dims=\"key\", coords={\"key\": frog_data.key}),\n",
    "                method=\"nearest\"\n",
    "            )\n",
    "            .to_dataframe()\n",
    "            .assign(val = lambda x: x.iloc[:, -1])\n",
    "            [['val']]\n",
    "            .reset_index()\n",
    "            .drop_duplicates()\n",
    "            .pivot(index=\"key\", columns=\"band\", values=\"val\")\n",
    "            .reset_index()\n",
    "        ),\n",
    "        on = ['key'],\n",
    "        how = 'inner'\n",
    "    )\n",
    "model_datas = []\n",
    "for frog_data, weather_data in zip(frog_datas, weather_datas):    \n",
    "    model_datas.append(join_frogs(frog_data, weather_data))\n",
    "\n",
    "model_data = pd.concat(model_datas).dropna()\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e366939f-d705-4fe3-b84c-824fa8ed585c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39572"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18298ec-5496-4169-8952-d2bc3076ac6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gbifID</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>stateProvince</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>species</th>\n",
       "      <th>key</th>\n",
       "      <th>1 ppt</th>\n",
       "      <th>...</th>\n",
       "      <th>7 tmax</th>\n",
       "      <th>7 tmin</th>\n",
       "      <th>8 ppt</th>\n",
       "      <th>8 soil</th>\n",
       "      <th>8 tmax</th>\n",
       "      <th>8 tmin</th>\n",
       "      <th>9 ppt</th>\n",
       "      <th>9 soil</th>\n",
       "      <th>9 tmax</th>\n",
       "      <th>9 tmin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occurrenceStatus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>...</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "      <td>25733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>...</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "      <td>13839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  gbifID  eventDate  country  continent  stateProvince  \\\n",
       "occurrenceStatus                                                         \n",
       "0                  25733      25733    25733      25733          25733   \n",
       "1                  13839      13839    13839      13839          13839   \n",
       "\n",
       "                  decimalLatitude  decimalLongitude  species    key  1 ppt  \\\n",
       "occurrenceStatus                                                             \n",
       "0                           25733             25733    25733  25733  25733   \n",
       "1                           13839             13839    13839  13839  13839   \n",
       "\n",
       "                  ...  7 tmax  7 tmin  8 ppt  8 soil  8 tmax  8 tmin  9 ppt  \\\n",
       "occurrenceStatus  ...                                                         \n",
       "0                 ...   25733   25733  25733   25733   25733   25733  25733   \n",
       "1                 ...   13839   13839  13839   13839   13839   13839  13839   \n",
       "\n",
       "                  9 soil  9 tmax  9 tmin  \n",
       "occurrenceStatus                          \n",
       "0                  25733   25733   25733  \n",
       "1                  13839   13839   13839  \n",
       "\n",
       "[2 rows x 57 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.groupby('occurrenceStatus').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb154fe-e72c-4e0f-b4ea-5a20384de3f5",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Now that we have the data in a format appropriate for machine learning, we can begin training a model. For this demonstration notebook, we will use a basic logistic regression model from the [scikit-learn](https://scikit-learn.org/stable/) library. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customisation capabilities.\n",
    "\n",
    "Scikit-learn models require separation of predictor variables and the response variable. We store the predictor variables in dataframe `X` and the response in the array `y`. We must make sure to drop the response variable from `X`, otherwise the model will have the answers! It also doesn't make sense to use latitude and longitude as predictor variables in such a confined area, so we drop those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06205583-fdd8-45ca-bc06-bad60a2fb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9dd94e-ba93-4667-897b-3923d3712567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(cache_size=1000, class_weight='balanced'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model = make_pipeline(StandardScaler(), SVC(class_weight='balanced', cache_size=1000))\n",
    "# Separate the predictor variables from the response\n",
    "X = (\n",
    "    model_data\n",
    "    .drop(['gbifID', 'eventDate', 'species',\n",
    "       'country', 'continent', 'stateProvince', 'occurrenceStatus', 'key', 'decimalLatitude', 'decimalLongitude'], 1)\n",
    ")\n",
    "y = model_data.occurrenceStatus.astype(int)\n",
    "\n",
    "# Fit model\n",
    "full_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565957c9-cba5-4948-84b6-c8f74f4cf598",
   "metadata": {},
   "source": [
    "### Model Prediction\n",
    "\n",
    "#### Predict Training Set\n",
    "\n",
    "Logistic regression is a machine learning model that estimates the probability of a binary response variable. In our case, the model will output the probability of a frog being present at a given location. To obtain the predictions for our training set, we simply use the `predict` method on our trained model. We will evaluate these predictions in the evaluation section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3825a5-2f5a-42c7-8cd3-ded68a80b757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 ppt</th>\n",
       "      <th>1 soil</th>\n",
       "      <th>1 tmax</th>\n",
       "      <th>1 tmin</th>\n",
       "      <th>10 ppt</th>\n",
       "      <th>10 soil</th>\n",
       "      <th>10 tmax</th>\n",
       "      <th>10 tmin</th>\n",
       "      <th>11 ppt</th>\n",
       "      <th>11 soil</th>\n",
       "      <th>...</th>\n",
       "      <th>7 tmax</th>\n",
       "      <th>7 tmin</th>\n",
       "      <th>8 ppt</th>\n",
       "      <th>8 soil</th>\n",
       "      <th>8 tmax</th>\n",
       "      <th>8 tmin</th>\n",
       "      <th>9 ppt</th>\n",
       "      <th>9 soil</th>\n",
       "      <th>9 tmax</th>\n",
       "      <th>9 tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.599998</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>26.860001</td>\n",
       "      <td>15.180000</td>\n",
       "      <td>56.200001</td>\n",
       "      <td>96.800003</td>\n",
       "      <td>20.220001</td>\n",
       "      <td>9.540001</td>\n",
       "      <td>62.400002</td>\n",
       "      <td>77.199997</td>\n",
       "      <td>...</td>\n",
       "      <td>13.700002</td>\n",
       "      <td>5.940001</td>\n",
       "      <td>81.199997</td>\n",
       "      <td>114.199997</td>\n",
       "      <td>14.120001</td>\n",
       "      <td>6.080003</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>16.540001</td>\n",
       "      <td>7.180002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.599998</td>\n",
       "      <td>56.599998</td>\n",
       "      <td>26.299999</td>\n",
       "      <td>14.120001</td>\n",
       "      <td>68.599998</td>\n",
       "      <td>157.399994</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>8.700002</td>\n",
       "      <td>78.199997</td>\n",
       "      <td>134.600006</td>\n",
       "      <td>...</td>\n",
       "      <td>13.540001</td>\n",
       "      <td>5.660004</td>\n",
       "      <td>99.199997</td>\n",
       "      <td>183.600006</td>\n",
       "      <td>13.820002</td>\n",
       "      <td>5.560004</td>\n",
       "      <td>87.599998</td>\n",
       "      <td>183.600006</td>\n",
       "      <td>16.080000</td>\n",
       "      <td>6.600001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.400002</td>\n",
       "      <td>46.799999</td>\n",
       "      <td>27.200001</td>\n",
       "      <td>14.719999</td>\n",
       "      <td>64.800003</td>\n",
       "      <td>137.199997</td>\n",
       "      <td>20.320002</td>\n",
       "      <td>8.960002</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>117.400002</td>\n",
       "      <td>...</td>\n",
       "      <td>13.760004</td>\n",
       "      <td>5.140001</td>\n",
       "      <td>87.400002</td>\n",
       "      <td>149.600006</td>\n",
       "      <td>14.260000</td>\n",
       "      <td>5.379999</td>\n",
       "      <td>82.599998</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>6.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46.400002</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>27.400003</td>\n",
       "      <td>14.900002</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>92.599998</td>\n",
       "      <td>20.500002</td>\n",
       "      <td>9.060002</td>\n",
       "      <td>74.199997</td>\n",
       "      <td>70.800003</td>\n",
       "      <td>...</td>\n",
       "      <td>13.480001</td>\n",
       "      <td>5.480000</td>\n",
       "      <td>90.800003</td>\n",
       "      <td>124.400002</td>\n",
       "      <td>14.000002</td>\n",
       "      <td>5.480002</td>\n",
       "      <td>82.599998</td>\n",
       "      <td>119.199997</td>\n",
       "      <td>16.480000</td>\n",
       "      <td>6.560002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.200001</td>\n",
       "      <td>16.200001</td>\n",
       "      <td>26.880001</td>\n",
       "      <td>15.580002</td>\n",
       "      <td>49.599998</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>20.640003</td>\n",
       "      <td>9.940002</td>\n",
       "      <td>52.400002</td>\n",
       "      <td>27.200001</td>\n",
       "      <td>...</td>\n",
       "      <td>14.040001</td>\n",
       "      <td>6.360001</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>14.720001</td>\n",
       "      <td>6.640002</td>\n",
       "      <td>57.799999</td>\n",
       "      <td>49.799999</td>\n",
       "      <td>17.160002</td>\n",
       "      <td>7.740001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>418.600006</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>32.159996</td>\n",
       "      <td>23.860003</td>\n",
       "      <td>76.400002</td>\n",
       "      <td>80.199997</td>\n",
       "      <td>35.340004</td>\n",
       "      <td>22.680002</td>\n",
       "      <td>172.800003</td>\n",
       "      <td>105.400002</td>\n",
       "      <td>...</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>16.500002</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>125.199997</td>\n",
       "      <td>32.440002</td>\n",
       "      <td>17.000002</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>95.400002</td>\n",
       "      <td>34.180000</td>\n",
       "      <td>20.040003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>413.000000</td>\n",
       "      <td>346.799988</td>\n",
       "      <td>32.180000</td>\n",
       "      <td>23.980000</td>\n",
       "      <td>72.599998</td>\n",
       "      <td>79.599998</td>\n",
       "      <td>35.200001</td>\n",
       "      <td>22.660002</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>90.400002</td>\n",
       "      <td>...</td>\n",
       "      <td>31.120001</td>\n",
       "      <td>15.740003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.800003</td>\n",
       "      <td>31.740002</td>\n",
       "      <td>16.540003</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>95.400002</td>\n",
       "      <td>34.080002</td>\n",
       "      <td>20.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>398.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>32.200001</td>\n",
       "      <td>23.980000</td>\n",
       "      <td>79.800003</td>\n",
       "      <td>75.599998</td>\n",
       "      <td>35.440002</td>\n",
       "      <td>22.800001</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>90.199997</td>\n",
       "      <td>...</td>\n",
       "      <td>32.020000</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>32.620003</td>\n",
       "      <td>17.240002</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>34.420002</td>\n",
       "      <td>20.160002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>335.000000</td>\n",
       "      <td>286.200012</td>\n",
       "      <td>32.579998</td>\n",
       "      <td>23.800001</td>\n",
       "      <td>56.400002</td>\n",
       "      <td>64.800003</td>\n",
       "      <td>36.160000</td>\n",
       "      <td>22.460001</td>\n",
       "      <td>143.600006</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>31.740002</td>\n",
       "      <td>15.240001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.400002</td>\n",
       "      <td>32.640003</td>\n",
       "      <td>16.200001</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>34.980000</td>\n",
       "      <td>19.700003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>465.200012</td>\n",
       "      <td>406.799988</td>\n",
       "      <td>32.040001</td>\n",
       "      <td>24.120001</td>\n",
       "      <td>79.800003</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>34.960003</td>\n",
       "      <td>23.100002</td>\n",
       "      <td>160.199997</td>\n",
       "      <td>104.199997</td>\n",
       "      <td>...</td>\n",
       "      <td>31.740002</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>139.800003</td>\n",
       "      <td>32.260002</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>106.199997</td>\n",
       "      <td>33.940002</td>\n",
       "      <td>20.480000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39572 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1 ppt      1 soil     1 tmax     1 tmin     10 ppt     10 soil  \\\n",
       "0      43.599998   32.000000  26.860001  15.180000  56.200001   96.800003   \n",
       "1      48.599998   56.599998  26.299999  14.120001  68.599998  157.399994   \n",
       "2      45.400002   46.799999  27.200001  14.719999  64.800003  137.199997   \n",
       "3      46.400002   29.400000  27.400003  14.900002  61.000000   92.599998   \n",
       "4      38.200001   16.200001  26.880001  15.580002  49.599998   38.799999   \n",
       "...          ...         ...        ...        ...        ...         ...   \n",
       "1099  418.600006  381.000000  32.159996  23.860003  76.400002   80.199997   \n",
       "1100  413.000000  346.799988  32.180000  23.980000  72.599998   79.599998   \n",
       "1101  398.000000  370.000000  32.200001  23.980000  79.800003   75.599998   \n",
       "1102  335.000000  286.200012  32.579998  23.800001  56.400002   64.800003   \n",
       "1103  465.200012  406.799988  32.040001  24.120001  79.800003   88.800003   \n",
       "\n",
       "        10 tmax    10 tmin      11 ppt     11 soil  ...     7 tmax     7 tmin  \\\n",
       "0     20.220001   9.540001   62.400002   77.199997  ...  13.700002   5.940001   \n",
       "1     19.700001   8.700002   78.199997  134.600006  ...  13.540001   5.660004   \n",
       "2     20.320002   8.960002   77.000000  117.400002  ...  13.760004   5.140001   \n",
       "3     20.500002   9.060002   74.199997   70.800003  ...  13.480001   5.480000   \n",
       "4     20.640003   9.940002   52.400002   27.200001  ...  14.040001   6.360001   \n",
       "...         ...        ...         ...         ...  ...        ...        ...   \n",
       "1099  35.340004  22.680002  172.800003  105.400002  ...  31.900000  16.500002   \n",
       "1100  35.200001  22.660002  151.000000   90.400002  ...  31.120001  15.740003   \n",
       "1101  35.440002  22.800001  161.000000   90.199997  ...  32.020000  16.740000   \n",
       "1102  36.160000  22.460001  143.600006   72.000000  ...  31.740002  15.240001   \n",
       "1103  34.960003  23.100002  160.199997  104.199997  ...  31.740002  17.100000   \n",
       "\n",
       "          8 ppt      8 soil     8 tmax     8 tmin      9 ppt      9 soil  \\\n",
       "0     81.199997  114.199997  14.120001   6.080003  72.800003  116.000000   \n",
       "1     99.199997  183.600006  13.820002   5.560004  87.599998  183.600006   \n",
       "2     87.400002  149.600006  14.260000   5.379999  82.599998  159.000000   \n",
       "3     90.800003  124.400002  14.000002   5.480002  82.599998  119.199997   \n",
       "4     69.000000   52.799999  14.720001   6.640002  57.799999   49.799999   \n",
       "...         ...         ...        ...        ...        ...         ...   \n",
       "1099   0.200000  125.199997  32.440002  17.000002  16.799999   95.400002   \n",
       "1100   0.000000  124.800003  31.740002  16.540003  13.400000   95.400002   \n",
       "1101   0.000000  114.000000  32.620003  17.240002  16.799999   88.800003   \n",
       "1102   0.000000   98.400002  32.640003  16.200001  11.000000   77.599998   \n",
       "1103   0.200000  139.800003  32.260002  17.540001  17.799999  106.199997   \n",
       "\n",
       "         9 tmax     9 tmin  \n",
       "0     16.540001   7.180002  \n",
       "1     16.080000   6.600001  \n",
       "2     16.620001   6.720000  \n",
       "3     16.480000   6.560002  \n",
       "4     17.160002   7.740001  \n",
       "...         ...        ...  \n",
       "1099  34.180000  20.040003  \n",
       "1100  34.080002  20.020000  \n",
       "1101  34.420002  20.160002  \n",
       "1102  34.980000  19.700003  \n",
       "1103  33.940002  20.480000  \n",
       "\n",
       "[39572 rows x 48 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b014512-659b-4f72-9102-b2b31c51bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = full_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f09ef4-142b-4f64-8ade-08160ccbf4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51835e-6061-4db0-9ad1-3a2510c6b271",
   "metadata": {},
   "source": [
    "#### Predict Entire Region\n",
    "\n",
    "For a species distribution model to be effective, it must also be capable of performing predictions over the entire region, not just the points in our training set. To do this, we will define another function called `predict_frogs` that will take our interpolated predictor variable image in, along with our logistic regression model, and output the probabilities for each pixel in the region. We will visualise these predictions in a heatmap in the results section of this notebook.\n",
    "\n",
    "This function will be used later to predict the test regions for the challenge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a306561-6f2b-4f0d-9ede-b98b702ac226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frogs(predictor_image, model):\n",
    "    \"\"\"Returns a (1, n, m) xarray where each pixel value corresponds to the probability of a frog occurrence.\n",
    "    \n",
    "    Takes in the multi-band image outputted by the `create_predictor_image` function as well as the\n",
    "    trained model and returns the predictions for each pixel value. Firstly, the $x$ and $y$ indexes\n",
    "    in the predictor image are stacked into one multi-index $z=(x, y)$ to produce an $k\\times n$\n",
    "    array, which is the format required to feed into our logistic regression model. Then, the array\n",
    "    is fed into the model, returning the model's predictions for the frog likelihood at each pixel. \n",
    "    The predicted probabilities are then indexed by the same multi-index $z$ as before, which allows \n",
    "    the array to be unstacked and returned as a one-band image, ready for plotting.\n",
    "\n",
    "    Arguments:\n",
    "    predictor_image -- (K, n, m) xarray, where K is the number of predictor variables.\n",
    "    model -- sklearn model with K predictor variables.\n",
    "    \"\"\"\n",
    "    # Stack up pixels so they are in the appropriate format for the model\n",
    "    predictor_image = predictor_image.stack(z=(\"y\", \"x\")).transpose()\n",
    "    # Reorder variables to be in same order as model\n",
    "    predictor_image = predictor_image.sel(band=model.feature_names_in_)\n",
    "    # Location of null values so that we can skip them (prediction model will break if nulls are present)\n",
    "    null_pixels = (np.sum(predictor_image.isnull(), axis=-1) > 0)\n",
    "    # Empty probabilities array\n",
    "    probabilities = np.zeros((len(null_pixels)))\n",
    "    # Calculate probability for each non-null pixel point\n",
    "    probabilities[~null_pixels] = model.predict(\n",
    "        predictor_image[~null_pixels]\n",
    "    )\n",
    "    # Set null pixels to a probability of null\n",
    "    probabilities[null_pixels] = np.nan\n",
    "    # Just take probability of frog (class=1)\n",
    "    # probabilities = probabilities[:,1]\n",
    "    # Add the coordinates to the probabilities, saving them in an xarray\n",
    "    resultant_image = xr.DataArray(\n",
    "        data=probabilities,\n",
    "        dims=['z'],\n",
    "        coords=dict(\n",
    "            z=predictor_image.z\n",
    "        )\n",
    "    )\n",
    "    # Unstack the image\n",
    "    resultant_image = resultant_image.unstack()\n",
    "    return resultant_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84b25f-bf7f-4583-bc09-7c740a0e97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability for each pixel point \n",
    "\n",
    "resultant_images = []\n",
    "for weather_data in weather_datas:\n",
    "    resultant_images.append(predict_frogs(weather_data, full_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca09d5-b7d1-42cf-80cb-66fc23225974",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Now that we have trained our model and made some predictions, all that is left is to evaluate it. We will do this by first visualising the output of the model with a probability heatmap. Then, we will evaluate both its in-sample and out-of-sample performance using the training set we have generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08badfa5-98bd-4962-b39a-a19966babfd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### In-Sample Evaluation\n",
    "\n",
    "In the last section, we made our predicitons for the training set and stored them in the `predictions` variable. We can now calculate some performance metrics to guage the effectiveness of the model. It must be stressed that this is the in-sample performance - the performance on the training set. Hence, the values will tend to overestimate its performance. Additionally, the training set itself is biased and this notebook only took naive approaches to address this. The model evaluation metrics are only as good as the data used to evaluate it, so the metrics themselves will also be biased. Thus, these metrics are NOT truly indicative of this model's performance. \n",
    "\n",
    "In this example, we will use `f1_score` and `accuracy_score` from Scikit-learn. Scikit-learn provides many other metrics that can be used for evaluation. You can even code your own if you think it will assist you in evaluating your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db621c-d83f-4ffc-ba7e-8579a79da9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 Score: {np.mean(f1_score(y, predictions)).round(2)}\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_score(y, predictions)).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ea862-0e9f-469f-8d18-f790cd738304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results in a confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(full_model, X, y, display_labels=['Absent', 'Present'], cmap='Blues')\n",
    "disp.figure_.set_size_inches((7, 7))\n",
    "disp.ax_.set_title('Model Level 1: Logistic\\nRegression Model In-Sample Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c1c74-e309-4fb5-9063-e680651f3bc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Probability Heatmap\n",
    "\n",
    "To create the probability heatmap, we write a function called `plot_heatmap`. This function will take in the model predictions from the entire region as stored in the `resultant_image` variable, and visualise these probabilities as a heatmap. In addition to the heatmap, we will also plot the actual map of the area in question, and the binary classification regions of the probability heatmap. The latter is simply a binary mask of the probability heatmap, 1 where the probability is greater than 0.5 and 0 elsewhere. \n",
    "\n",
    "To help visualise the effectiveness of our model, we plot the target species occurrences over top of each image. This can give us an idea of where our model is doing well, and where it is doing poorly. Particularly, we are interested in the high false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb12b7-395d-4741-8ed6-0487f6d611d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(resultant_image, frog_data, title, crs = {'init':'epsg:4326'}):\n",
    "    \"\"\"Plots a real map, probability heatmap, and model classification regions for the probability image from our model.\n",
    "\n",
    "    Arguments:\n",
    "    resultant_image -- (1, n, m) xarray of probabilities output from the model\n",
    "    frog_data -- Dataframe of frog occurrences, indicated with a 1 in the occurrenceStatus column. \n",
    "                 Must contain [\"occurrenceStatus\", \"decimalLongitude\", \"decimalLatitude\"]\n",
    "    title -- string that will be displayed as the figure title\n",
    "    crs -- coordinate reference system for plotting the real map. Defaults to EPSG:4326.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (20, 10), sharex=True, sharey=True)\n",
    "    extent = [resultant_image.x.min(),resultant_image.x.max(),resultant_image.y.min(),resultant_image.y.max()]\n",
    "    cmap = 'PiYG'\n",
    "\n",
    "    # Plot real map\n",
    "    ax[0].scatter(x=[extent[0], extent[1]], y=[extent[2], extent[3]], alpha=0)\n",
    "    cx.add_basemap(ax[0], crs=crs)\n",
    "    ax[0].set_title('Real map')\n",
    "    \n",
    "    # Plot heatmap from model\n",
    "    heatmap = resultant_image.plot.imshow(\n",
    "        x='x', y='y', ax=ax[1], cmap=cmap, vmin=0, vmax=1, interpolation='none', add_colorbar=False\n",
    "    )\n",
    "    ax[1].set_aspect('equal')\n",
    "    ax[1].set_title('Model Probability Heatmap')\n",
    "\n",
    "    # Plot binary classification from model\n",
    "    regions = xr.where(resultant_image.isnull(), np.nan, resultant_image>0.5).plot.imshow(\n",
    "            x='x', y='y', ax=ax[2], cmap=cmap, vmin=0, vmax=1, interpolation='none', add_colorbar=False\n",
    "    )\n",
    "    ax[2].set_aspect('equal')\n",
    "    ax[2].set_title('Model Classification Regions')\n",
    "\n",
    "    # Plot real frogs\n",
    "    for i, axis in enumerate(ax):\n",
    "        filt = frog_data.occurrenceStatus == 1\n",
    "        axis.scatter(\n",
    "            frog_data[filt].decimalLongitude, frog_data[filt].decimalLatitude, \n",
    "            color = 'dodgerblue', marker='.', alpha=0.5, label='Target Species' if i==0 else ''\n",
    "        )\n",
    "\n",
    "    fig.colorbar(heatmap, ax=ax, location = 'bottom', aspect=40)\n",
    "    fig.legend(loc = (0.9, 0.9))\n",
    "    fig.suptitle(title, x=0.5, y=0.9, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f408723-dcd7-43c6-a91a-f27f4481f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frog_data, resultant_image in zip(frog_datas, resultant_images):\n",
    "    plot_heatmap(resultant_image, frog_data, \"Logistic Regression Model Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d77515-bbdc-48aa-b21f-5dd6aff7a52c",
   "metadata": {},
   "source": [
    "#### Out-of-Sample Evaluation\n",
    "\n",
    "When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalise. This is because models have a tendancy to overfit the dataset they are trained on. To estimate the out-of-sample performance, we will use k-fold cross-validation. This technique involves splitting the training dataset into folds, in this case we will use 10. Each iteration, the model is trained on all but one of the folds, which is reserved for testing. This is repeated until all folds have been left out once. At the end of the process, we will have 10 metrics which can be averaged, giving a more reliable and valid measure of model performance. \n",
    "\n",
    "Scikit-learn has built-in functions that can assist in k-fold cross validation. In particular, we will use `StratifiedKFold` to split our data into folds, ensuring there is always a balanced number of frogs and non-frogs in each fold.\n",
    "\n",
    "Again, these metrics are derived from a biased sample, so be careful what you infer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e992d-08cd-4ce6-9330-852263bd0b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_model = LogisticRegression()\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, random_state=420, shuffle=True)\n",
    "metrics = {'F1': f1_score, 'Accuracy': accuracy_score}\n",
    "results = {'predicted':[], 'actual':[]}\n",
    "scores = {'F1': [], 'Accuracy': []}\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    # Split the dataset\n",
    "    print(f\"Fold {i+1} of {n_folds}\")\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model with the training set\n",
    "    cv_model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = cv_model.predict(X_test)\n",
    "    \n",
    "    for metric, fn in metrics.items():\n",
    "        scores[metric].append(fn(y_test, predictions))\n",
    "        \n",
    "    results['predicted'].extend(predictions)\n",
    "    results['actual'].extend(list(y_test))\n",
    "        \n",
    "print(f'\\nMetrics averaged over {n_folds} trials:')\n",
    "for metric, result in scores.items():\n",
    "    print(f\"{metric}: {np.mean(result).round(2)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239a36d-4d40-4ce8-ace4-ccad12bc0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results in a confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(results['actual'], results['predicted'], display_labels=['Absent', 'Present'], cmap='Blues')\n",
    "disp.figure_.set_size_inches((7, 7))\n",
    "disp.ax_.set_title('Model Level 1: Logistic Regression Model\\n10-fold Cross Validation Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddcdfe-309f-43e5-862a-24497d12450e",
   "metadata": {},
   "source": [
    "The results from the 10-fold cross validation are similar than the in-sample metrics. This is a good sign as it shows that we haven't overfit our model. We see similar behavour in the higher rate of false positives that we saw in the in-sample performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca134a4-7445-4097-ad62-574453c00c21",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you are happy with your model, there will come a time to make a submission to the challenge. To make a submission, you will need to use your model to make predictions about the presence of litoria fallax for a set of test coordinates we have provided. The coordinates are found in the 'challenge_1_submission_template.csv' file, and the list of bounding boxes where the points were sampled from can be found separately in the 'challenge_1_test_regions.txt' file. We recommend looping through the regions identified in that file, pulling the TerraClimate data for that region, and extracting the features for each point in the 'test_1_occurrences.csv' file within that regions bounding box. This will minimise the computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acebfca-8e59-4fbb-b87e-9c61406d9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in test coordinates\n",
    "test_file = pd.read_csv('challenge_1_submission_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f923285-32ad-4dfb-8595-1c80e3aebf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test regions\n",
    "test_1_regions = []\n",
    "with open('challenge_1_test_regions.txt', 'r') as file: \n",
    "    for i, line in enumerate(file):\n",
    "        if i > 0:\n",
    "            test_1_regions.append(eval(\"(\"+line+\")\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dd01f-4802-44d8-bd30-7ca18aa51cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in regions and save as list of dictionaries.\n",
    "test_regions = [{'title':i, 'bbox':bbox} for i, bbox in enumerate(test_1_regions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909537b-fc16-40d5-bd2e-213e47826091",
   "metadata": {},
   "source": [
    "Note: with the TerraClimate parameters we have set, some areas of each region contain nulls. If this is the case, the prediction will return a null, which evaluates to false when we create the binary mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c645d-9cf7-4aaa-a93f-a3d1f16fda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictor data for each region and get predictor image\n",
    "for region in test_regions:\n",
    "    region['coords'] = filter_bbox(test_file[['id', 'decimalLongitude', 'decimalLatitude']], region['bbox'])\n",
    "    region['predictors'] = get_terraclimate(region['bbox'], time_slice=time_slice, assets=assets)\n",
    "    region['result'] = predict_frogs(region['predictors'], full_model) > 0.5\n",
    "    \n",
    "    region['result'].plot.imshow(x='x', y='y', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bec83a-6b6b-40b9-b869-87dce4275b80",
   "metadata": {},
   "source": [
    "We can now use these classification regions to assign predictions for each of the coordinates specified in the test file. We do this in a similar way to the `join_frogs` function we defined earlier, except in this case we are joining a prediction to each coordinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe031cf-400d-40e8-adc1-d2df9e8b4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "\n",
    "for region in test_regions:\n",
    "    preds = (\n",
    "        region['result'].rename('occurrenceStatus')\n",
    "        .sel(\n",
    "            x=xr.DataArray(region['coords'].decimalLongitude, dims=\"id\", coords={\"id\": region['coords'].id}), \n",
    "            y=xr.DataArray(region['coords'].decimalLatitude, dims=\"id\", coords={\"id\": region['coords'].id}),\n",
    "            method=\"nearest\"\n",
    "        )\n",
    "        .to_dataframe()\n",
    "        .reset_index()\n",
    "        .rename(columns={'x':'decimalLongitude', 'y':'decimalLatitude'})\n",
    "    )\n",
    "    predictions = predictions.append(preds)\n",
    "            \n",
    "submission = (    \n",
    "    predictions.merge(\n",
    "        test_file, \n",
    "        on=['decimalLongitude', 'decimalLatitude'], \n",
    "        how='left', suffixes = ('', '_'))\n",
    "    [test_file.columns]\n",
    "    .fillna(0)\n",
    "    .astype({col:'int' for col in test_file.columns[3::]})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b624948-932e-4b06-a23b-72a13767a705",
   "metadata": {},
   "source": [
    "What we are left with is a submission file with three columns: decimalLatitude, decimalLongitude, and occurrenceStatus. This is the file you will submit to the EY Data Science Challenge platform to receive your score on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60ccee-1fa5-4b35-a6c8-069d9cea33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(submission)\n",
    "\n",
    "# Save to output folder\n",
    "submission.to_csv('challenge_1_submission_LX_13.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41938b59-c2be-454e-be3a-80251330aa62",
   "metadata": {},
   "source": [
    "### Get Frogging!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec962f-89f9-465e-9269-73374840dcb8",
   "metadata": {},
   "source": [
    "Now that you have witnessed a basic approach to model training, its time to try your own approach! Feel free to modify any of the functions presented in this notebook. A good start might be to try running this notebook on a region different to Greater Sydney, or even on multiple regions.\n",
    "\n",
    "Be sure to address some of the assumptions made here, particularly ways to address the sampling bias in the dataset. Our pseudo-absence method was just one idea, you may want to persue another. Another important issue to consider is that of class imbalance. In this notebook, we simply down-sampled the non-target species to match the number of target species. This may not be ideal, as an isolated frog occurrence may be lost while clusters of occurrences are more likely to persist. Perhaps a method of sampling only from clustered occurrences would address class imbalance while also helping to offset the sampling bias. Just a thought! You might even decide on a completely different training set, such as classifying regions rather than points. Do whatever you think will create the best model for predicting the frog habitat of the species of interest. Happy frogging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759de91f-2ce6-470f-a858-9d1cc764fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('challenge_1_submission_LX_13.csv')\n",
    "df_benchmark = pd.read_csv('challenge_1_submission_benchmark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb5194-aae1-4686-86d4-93f0509ed5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df != df_benchmark)['occurrenceStatus'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229c6c2-6fd1-46bf-8d3b-ea106a4d5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other = pd.read_csv('challenge_1_submission_LX_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4155e6-6c65-4541-aec2-16cec9684c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df != df_other)['occurrenceStatus'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3be98d-be06-44b2-b459-33cde907f64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2022DSC",
   "language": "python",
   "name": "conda-env-2022DSC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
